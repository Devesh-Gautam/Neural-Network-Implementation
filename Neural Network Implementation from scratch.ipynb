{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4241577",
   "metadata": {},
   "source": [
    "### using numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fe59d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3922e257",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [1, 2, 3, 2.5]\n",
    "weights = [\n",
    "    [0.2, 0.8, -0.5, 1.0],\n",
    "    [0.5, -0.91, 0.26, -0.5],\n",
    "    [-0.26, -0.27, 0.17, 0.87]]\n",
    "biases = [2, 3, 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b410b62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here it is must to write weights, inputs we cant write np.dot(inputs, weights)\n",
    "print(\"Layer_outputs: \", np.dot(np.array(weights), np.array(inputs)) + biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5de26e",
   "metadata": {},
   "source": [
    "passing a batch(2 inputs) of data to layer of neurons(3 neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e254ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.array([[1, 2, 3, 2.5],[-1,-2,-3,-2.5]])\n",
    "weights = [\n",
    "    [0.2, 0.8, -0.5, 1.0],\n",
    "    [0.5, -0.91, 0.26, -0.5],\n",
    "    [-0.26, -0.27, 0.17, 0.87]]\n",
    "biases = [2, 3, 0.5]\n",
    "\n",
    "print(\"Layer_outputs:\\n\", np.dot(inputs, np.array(weights).T) + biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388b9770",
   "metadata": {},
   "source": [
    "#### Coding multiple layers of neurons and stacking them together\n",
    "\n",
    "input has 4 features, hidden layer 1 of 3 neurons, hidden layer 2 of 3 neurons "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3789f865",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [[1, 2, 3, 2.5],[-1,-2,-3,-2.5]]\n",
    "\n",
    "weights1 = [\n",
    "    [0.2, 0.8, -0.5, 1.0],\n",
    "    [0.5, -0.91, 0.26, -0.5],\n",
    "    [-0.26, -0.27, 0.17, 0.87]]\n",
    "biases1 = [2, 3, 0.5]\n",
    "\n",
    "\n",
    "weights2 = [\n",
    "    [0.1, -0.14, 0.5],\n",
    "    [-0.5, 0.12, -0.33],\n",
    "    [-0.44, 0.73, -0.13]]\n",
    "biases2 = [-1, 2, -0.5]\n",
    "\n",
    "# Using numpy arrays for matrix operations\n",
    "input_array = np.array(inputs)\n",
    "weights1_array = np.array(weights1)\n",
    "biases1_array = np.array(biases1)\n",
    "weights2_array = np.array(weights2)\n",
    "biases2_array = np.array(biases2)\n",
    "\n",
    "layer_outputs1 = np.dot(input_array, weights1_array.T) + biases1\n",
    "print(layer_outputs1)\n",
    "layer_outputs2 = np.dot(layer_outputs1, weights2_array.T) + biases2\n",
    "layer_outputs2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56796ce6",
   "metadata": {},
   "source": [
    "## - Implementing the dense layer class in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ca6d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nnfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a1291b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating non linear data\n",
    "# inport nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "import matplotlib.pyplot as plt\n",
    "# nnfs.init()\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='brg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebbbb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense Layer Class\n",
    "\n",
    "# creating a class for dog to get handy with classes\n",
    "class Dog:\n",
    "    def __init__(self, name, age):\n",
    "        self.nameDog = name\n",
    "        self.age = age\n",
    "\n",
    "    def bark(self):\n",
    "        print(f\"{self.nameDog} says Woof!\")\n",
    "\n",
    "# creating an instance of the Dog class\n",
    "my_dog = Dog(\"Buddy\", 3)\n",
    "my_dog.bark()  # calling the bark method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11a6000",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = np.random.randn(n_inputs, n_neurons) * 0.01 # small random values\n",
    "        self.biases = np.zeros((1, n_neurons)) \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Calculate the output of the layer from inputs, weights, and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "    \n",
    "# Example usage\n",
    "layer1 = Layer_Dense(4, 5)  # 4 inputs, 5 neurons\n",
    "layer1.forward(np.array([[1, 2, 3, 2.5], [-1, -2, -3, -2.5]]))\n",
    "print(\"Layer 1 output:\\n\", layer1.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943ba9c4",
   "metadata": {},
   "source": [
    "## - Broadcasting and array summation in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd7b7a3",
   "metadata": {},
   "source": [
    "#### Summation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7894068",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = [[1, 2, 3, 2.5], [-1, -2, -3, -2.5]]\n",
    "arr = np.array(arr)\n",
    "\n",
    "print(arr.sum()) # or arr.sum(axis=None)  # sum of all elements\n",
    "print(arr.sum(axis=0))  # sum along columns\n",
    "print(arr.sum(axis=1))  # sum along rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fb7a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use of keepdims - to keep the dimensions of the array\n",
    "print(arr.sum(axis=0, keepdims=True))  # sum along columns with dimensions kept\n",
    "print(arr.sum(axis=1, keepdims=True))  # sum along rows with dimensions kept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316ce426",
   "metadata": {},
   "source": [
    "### Broadcasting rules\n",
    "\n",
    "1. If the dimensions of the arrays are equal, they are compatible.\n",
    "2. If one of the dimensions is 1, it can be stretched to match the other dimension.\n",
    "3. If one of the dimensions does not exist, it can be treated as 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025e2094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subtarct the max of the row from the row\n",
    "arr = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "\n",
    "print(\"Original array:\\n\", arr)\n",
    "print(\"Wrong answer if keepdims=False:\\n\", arr - arr.max(axis=1))  # this will not work as expected\n",
    "print(\"Correct answer with keepdims=True:\\n\", arr - arr.max(axis=1, keepdims=True))  # this will work as expected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4b5570",
   "metadata": {},
   "source": [
    "##  - Coding Neural Network Activation Functions from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb54612e",
   "metadata": {},
   "source": [
    "#### we need to implement activation functions so that we can bring non-linearity to the model otherwise it will be just a linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f86a38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# usage of dense layer \n",
    "layer1 = Layer_Dense(3,5) #3 inputs, 5 neurons\n",
    "print(\"Layer 1 weights:\\n\", layer1.weights)\n",
    "print(\"Layer 1 biases:\\n\", layer1.biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586ea004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU activation function is a popular activation function in neural networks that introduces non-linearity by outputting the input directly if it is positive; otherwise, it outputs zero.\n",
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        # Apply ReLU activation function\n",
    "        self.output = np.maximum(0, inputs)\n",
    "        \n",
    "# Leaky ReLU activation function is a variant of ReLU that allows a small, non-zero gradient when the input is negative, helping to mitigate the \"dying ReLU\" problem.\n",
    "class Activation_Leaky_ReLU:\n",
    "    def __init__(self, alpha=0.01):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Apply Leaky ReLU activation function\n",
    "        self.output = np.where(inputs > 0, inputs, inputs * self.alpha)\n",
    "\n",
    "# ELU (Exponential Linear Unit) activation function is another variant that smooths the output for negative inputs, which can help with learning.\n",
    "class Activation_ELU:\n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Apply ELU activation function\n",
    "        self.output = np.where(inputs > 0, inputs, self.alpha * (np.exp(inputs) - 1))\n",
    "\n",
    "# Sigmoid activation function is often used in the output layer of binary classification problems.\n",
    "class Activation_Sigmoid:\n",
    "    def forward(self, inputs):\n",
    "        # Apply Sigmoid activation function\n",
    "        self.output = 1 / (1 + np.exp(-inputs))\n",
    "\n",
    "# Tanh activation function is another common activation function that outputs values between -1 and 1, providing a zero-centered output.\n",
    "class Activation_Tanh:\n",
    "    def forward(self, inputs):\n",
    "        # Apply Tanh activation function\n",
    "        self.output = np.tanh(inputs)\n",
    "\n",
    "\n",
    "# Softmax activation function is typically used in the output layer of a neural network for multi-class classification problems.\n",
    "class Activation_Softmax:\n",
    "    def forward(self, inputs):\n",
    "        # Apply Softmax activation function\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))  # for numerical stability\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        self.output = probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0600a3",
   "metadata": {},
   "source": [
    "Coding a forward layer with 2 hlayers and 2 activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbe071a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dataset \n",
    "# here X is a 2D array with shape (300, 2) and y is a 1D array with shape (300,)\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "print(X.shape, y.shape)  # should print (300, 2) (300,)\n",
    "\n",
    "# pass the data through the layers\n",
    "# First, we will create a dense layer with 2 input features and 3 neurons,\n",
    "layer1 = Layer_Dense(2, 3)  # 2 inputs_features, 5 neurons\n",
    "layer1.forward(X)\n",
    "layer1_output = layer1.output\n",
    "print(\"Layer 1 output:\\n\", layer1_output[:5])  # print first 5 outputs for brevity\n",
    "\n",
    "# Now, we will apply the ReLU activation function to the output of the first layer\n",
    "activation1 = Activation_ReLU()\n",
    "activation1.forward(layer1_output)\n",
    "activation1_output = activation1.output\n",
    "print(\"Activation_Layer 1 output:\\n\", activation1_output[:5])  # print first 5 outputs for brevity\n",
    "\n",
    "# Next, we will create another dense layer with 3 inputs from the previous layer and 3 neurons,\n",
    "layer2 = Layer_Dense(3, 3)  # 3 inputs from previous layer, 3 neurons\n",
    "layer2.forward(activation1_output)\n",
    "layer2_output = layer2.output\n",
    "print(\"Layer 2 output:\\n\", layer2_output[:5])  # print first 5 outputs for brevity\n",
    "\n",
    "# Finally, we will apply the Softmax activation function to the output of the second layer\n",
    "activation2 = Activation_Softmax()\n",
    "activation2.forward(layer2_output)\n",
    "activation2_output = activation2.output\n",
    "print(\"Activation_Layer 2 output:\\n\", activation2_output[:5])  # print first 5 outputs for brevity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e14978",
   "metadata": {},
   "source": [
    "##  - Coding the cross entropy loss in Python (from scratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc605bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical cross entropy loss function is commonly used in multi-class classification problems.\n",
    "\n",
    "softmax_outputs = np.array([\n",
    "    [0.7, 0.1, 0.2],\n",
    "    [0.1, 0.5, 0.4],\n",
    "    [0.02, 0.9, 0.08]\n",
    "])\n",
    "\n",
    "class_targets  =  [0, 1, 1]  # 0th class for first sample, 1st class for second and third samples\n",
    "\n",
    "# Usinf numpy advanced indexing to get the probabilities of the target classes\n",
    "print(softmax_outputs[[0,1,2], class_targets])\n",
    "\n",
    "neg_log = -np.log(softmax_outputs[[0,1,2], class_targets])\n",
    "print(\"Negative log probabilities of target classes:\\n\", neg_log)\n",
    "print(\"Categorical cross entropy loss:\\n\", np.mean(neg_log))  # mean of negative log probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b1102b",
   "metadata": {},
   "source": [
    "Let me explain why we use the mean instead of sum in categorical cross entropy loss:\n",
    "\n",
    "### Why Mean vs Sum?\n",
    "\n",
    "1. **Batch Size Independence**\n",
    "   - Using mean makes the loss value **independent of batch size**\n",
    "   - If we used sum, the loss would scale directly with batch size\n",
    "   - This would make it harder to:\n",
    "     - Compare losses between different batch sizes\n",
    "     - Set consistent learning rates\n",
    "     - Define stable convergence criteria\n",
    "\n",
    "2. **Example to Illustrate**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ae8113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume we have same data split into different batch sizes\n",
    "batch_1 = [-np.log(0.7), -np.log(0.5), -np.log(0.9)]  # batch_size = 3\n",
    "batch_2 = [-np.log(0.7), -np.log(0.5)]  # batch_size = 2\n",
    "\n",
    "# Using sum\n",
    "sum_loss_1 = np.sum(batch_1)  # larger value due to more samples\n",
    "sum_loss_2 = np.sum(batch_2)  # smaller value due to fewer samples\n",
    "\n",
    "# Using mean\n",
    "mean_loss_1 = np.mean(batch_1)  # comparable value regardless of batch size\n",
    "mean_loss_2 = np.mean(batch_2)  # comparable value regardless of batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d713477",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "3. **Training Stability**\n",
    "   - Mean loss provides more stable gradients during training\n",
    "   - Helps maintain consistent update steps regardless of batch size\n",
    "   - Makes hyperparameter tuning more manageable\n",
    "\n",
    "4. **Mathematical Interpretation**\n",
    "   - Mean represents the expected loss per sample\n",
    "   - This aligns better with the probabilistic interpretation of cross-entropy\n",
    "   - Gives a more intuitive measure of model performance\n",
    "\n",
    "That said, you can use sum if you prefer, but you'd need to adjust other hyperparameters (like learning rate) to compensate for different batch sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a2a055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if classes are one-hot encoded\n",
    "one_hot_targets = np.array([\n",
    "    [1, 0, 0],\n",
    "    [0, 1, 0],\n",
    "    [0, 1, 0]\n",
    "])\n",
    "\n",
    "A = softmax_outputs * one_hot_targets\n",
    "B = np.sum(A, axis=1)\n",
    "C = -np.log(B)\n",
    "print(C)\n",
    "print(\"Categorical cross entropy loss with one-hot encoded targets:\\n\", np.mean(C))  # mean of negative log probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40645dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing the Categorical Cross Entropy loss function in a class structure\n",
    "class Loss:\n",
    "    def calculate(self, y_pred, y_true):\n",
    "        # Calculate the loss value\n",
    "        sample_losses = self.forward(y_pred, y_true)\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        return data_loss\n",
    "  \n",
    "# Categorical cross entropy loss class\n",
    "# This class inherits from the Loss class and implements the forward method to calculate the categorical cross entropy\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    # forward pass to calculate the loss\n",
    "    def forward(self, y_pred, y_true):\n",
    "        \n",
    "        samples = len(y_pred)\n",
    "        # clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)  # to avoid log(0)\n",
    "        \n",
    "        if len(y_true.shape) == 1:\n",
    "            # if y_true is a 1D array (class labels)\n",
    "            correct_confidences = y_pred_clipped[range(samples), y_true]\n",
    "        elif len(y_true.shape) == 2:\n",
    "            # if y_true is a 2D array (one-hot encoded)\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)\n",
    "        \n",
    "        # losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return (negative_log_likelihoods) # return the loss values\n",
    "\n",
    "# Example usage of the Loss_CategoricalCrossentropy class\n",
    "loss_function = Loss_CategoricalCrossentropy()      \n",
    "y_pred = np.array([[0.7, 0.1, 0.2],\n",
    "                   [0.1, 0.5, 0.4],\n",
    "                   [0.02, 0.9, 0.08]])\n",
    "y_true = np.array([0, 1, 1])  # class labels\n",
    "loss_value = loss_function.calculate(y_pred, y_true)\n",
    "print(\"Categorical cross entropy loss value:\", loss_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ce55c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete forward pass with a simple neural network\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "layer1 = Layer_Dense(2, 5)  # 2 inputs, 5 neurons\n",
    "activation1 = Activation_ReLU()\n",
    "layer2 = Layer_Dense(5, 3)  # 5 inputs from previous layer, 3 neurons\n",
    "activation2 = Activation_Softmax()\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "\n",
    "layer1.forward(X)\n",
    "activation1.forward(layer1.output)\n",
    "layer2.forward(activation1.output)\n",
    "activation2.forward(layer2.output)\n",
    "print(\"Final output of the forward pass:\\n\", activation2.output[:5])  # print first 5 outputs for brevity\n",
    "# calculating the loss using the Loss_CategoricalCrossentropy class\n",
    "loss_value = loss_function.calculate(activation2.output, y)\n",
    "print(\"Loss value after forward pass:\", loss_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94118725",
   "metadata": {},
   "source": [
    "#### Accuracy - another metric to compare output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6943f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.argmax(activation2.output, axis=1)  # get the predicted class labels\n",
    "print(\"Predicted class labels:\", predictions[:5])  # print first 5 predictions for\n",
    "print(\"Accuracy calculation:\", np.mean(predictions == y))  # calculate accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b56d79",
   "metadata": {},
   "source": [
    "## Optimisation of model, loss reduction, Back Propagation, Weight Updation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5705aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SIMPLER DATASET\n",
    "import matplotlib.pyplot as plt\n",
    "import nnfs\n",
    "from nnfs.datasets import vertical_data\n",
    "nnfs.init()\n",
    "X, y = vertical_data(samples=100, classes=3)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap='brg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b1b1ab",
   "metadata": {},
   "source": [
    "Randomly selecting weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6788e0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "X, y = vertical_data(samples=100, classes=3)\n",
    "# Create model\n",
    "dense1 = Layer_Dense(2, 3) # first dense layer, 2 inputs\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = Layer_Dense(3, 3) # second dense layer, 3 inputs, 3 outputs\n",
    "activation2 = Activation_Softmax()\n",
    "# Create loss function\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Helper variables\n",
    "lowest_loss = 9999999 # some initial value\n",
    "best_dense1_weights = dense1.weights.copy()\n",
    "best_dense1_biases = dense1.biases.copy()\n",
    "best_dense2_weights = dense2.weights.copy()\n",
    "best_dense2_biases = dense2.biases.copy()\n",
    "\n",
    "for iteration in range(100000):\n",
    " # Generate a new set of weights for iteration\n",
    " dense1.weights = 0.05 * np.random.randn(2, 3)\n",
    " dense1.biases = 0.05 * np.random.randn(1, 3)\n",
    " dense2.weights = 0.05 * np.random.randn(3, 3)\n",
    " dense2.biases = 0.05 * np.random.randn(1, 3)\n",
    " # Perform a forward pass of the training data through this layer\n",
    " dense1.forward(X)\n",
    " activation1.forward(dense1.output)\n",
    " dense2.forward(activation1.output)\n",
    " activation2.forward(dense2.output)\n",
    " # Perform a forward pass through activation function\n",
    " # it takes the output of second dense layer here and returns loss\n",
    " loss = loss_function.calculate(activation2.output, y)\n",
    " # Calculate accuracy from output of activation2 and targets\n",
    " # calculate values along first axis\n",
    " predictions = np.argmax(activation2.output, axis=1)\n",
    " accuracy = np.mean(predictions == y)\n",
    " # If loss is smaller - print and save weights and biases aside\n",
    " if loss < lowest_loss:\n",
    "   print('New set of weights found, iteration:', iteration,'loss:', loss, 'acc:', accuracy)\n",
    "   best_dense1_weights = dense1.weights.copy()\n",
    "   best_dense1_biases = dense1.biases.copy()\n",
    "   best_dense2_weights = dense2.weights.copy()\n",
    "   best_dense2_biases = dense2.biases.copy()\n",
    "   lowest_loss = loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b6c183",
   "metadata": {},
   "source": [
    "Randomly adjusting weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8140e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "X, y = vertical_data(samples=100, classes=3)\n",
    "# Create model\n",
    "dense1 = Layer_Dense(2, 3) # first dense layer, 2 inputs\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = Layer_Dense(3, 3) # second dense layer, 3 inputs, 3 outputs\n",
    "activation2 = Activation_Softmax()\n",
    "# Create loss function\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "# Helper variables\n",
    "lowest_loss = 9999999 # some initial value\n",
    "best_dense1_weights = dense1.weights.copy()\n",
    "best_dense1_biases = dense1.biases.copy()\n",
    "best_dense2_weights = dense2.weights.copy()\n",
    "best_dense2_biases = dense2.biases.copy()\n",
    "for iteration in range(10000):\n",
    " # Update weights with some small random values\n",
    " dense1.weights += 0.05 * np.random.randn(2, 3)\n",
    " dense1.biases += 0.05 * np.random.randn(1, 3)\n",
    " dense2.weights += 0.05 * np.random.randn(3, 3)\n",
    " dense2.biases += 0.05 * np.random.randn(1, 3)\n",
    " # Perform a forward pass of our training data through this layer\n",
    " dense1.forward(X)\n",
    " activation1.forward(dense1.output)\n",
    " dense2.forward(activation1.output)\n",
    " activation2.forward(dense2.output)\n",
    " # Perform a forward pass through activation function\n",
    " # it takes the output of second dense layer here and returns loss\n",
    " loss = loss_function.calculate(activation2.output, y)\n",
    " # Calculate accuracy from output of activation2 and targets\n",
    " # calculate values along first axis\n",
    " predictions = np.argmax(activation2.output, axis=1)\n",
    " accuracy = np.mean(predictions == y)\n",
    " # If loss is smaller - print and save weights and biases aside\n",
    " if loss < lowest_loss:\n",
    "  print('New set of weights found, iteration:', iteration,'loss:', loss, 'acc:', accuracy)\n",
    "  best_dense1_weights = dense1.weights.copy()\n",
    "  best_dense1_biases = dense1.biases.copy()\n",
    "  best_dense2_weights = dense2.weights.copy()\n",
    "  best_dense2_biases = dense2.biases.copy()\n",
    "  lowest_loss = loss\n",
    " # Revert weights and biases\n",
    " else:\n",
    "  dense1.weights = best_dense1_weights.copy()\n",
    "  dense1.biases = best_dense1_biases.copy()\n",
    "  dense2.weights = best_dense2_weights.copy()\n",
    "  dense2.biases = best_dense2_biases.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193e352a",
   "metadata": {},
   "source": [
    "Not works in complex datasets like spiral data\n",
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dcec39",
   "metadata": {},
   "source": [
    "Gradient loss with respect to weights is X.T dot dL_dz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782e4e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Passed-in gradient from the next layer\n",
    "# for the purpose of this example we're going to use\n",
    "# an array of an incremental gradient values\n",
    "dvalues = np.array([[1., 1., 1.],\n",
    " [2., 2., 2.],\n",
    " [3., 3., 3.]])\n",
    "# We have 3 sets of inputs - samples\n",
    "inputs = np.array([[1, 2, 3, 2.5],\n",
    " [2., 5., -1., 2],\n",
    " [-1.5, 2.7, 3.3, -0.8]])\n",
    "# sum weights of given input\n",
    "# and multiply by the passed-in gradient for this neuron\n",
    "dweights = np.dot(inputs.T, dvalues)\n",
    "print(dweights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393ccc31",
   "metadata": {},
   "source": [
    "Gradients of the loss with respect to biases -- sum rows of dL_dz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a63c042",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Passed-in gradient from the next layer\n",
    "# for the purpose of this example we're going to use\n",
    "# an array of an incremental gradient values\n",
    "dvalues = np.array([[1., 1., 1.],\n",
    " [2., 2., 2.],\n",
    " [3., 3., 3.]])\n",
    "# One bias for each neuron\n",
    "# biases are the row vector with a shape (1, neurons)\n",
    "biases = np.array([[2, 3, 0.5]])\n",
    "# dbiases - sum values, do this over samples (first axis), keepdims\n",
    "# since this by default will produce a plain list -\n",
    "# we explained this in the chapter 4\n",
    "dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "print(dbiases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cc65f0",
   "metadata": {},
   "source": [
    "GRADIENTS OF THE LOSS WITH RESPECT TO INPUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014fcb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Passed-in gradient from the next layer\n",
    "# for the purpose of this example we're going to use\n",
    "# an array of an incremental gradient values\n",
    "dvalues = np.array([[1., 1., 1.],\n",
    " [2., 2., 2.],\n",
    " [3., 3., 3.]])\n",
    "# We have 3 sets of weights - one set for each neuron\n",
    "# we have 4 inputs, thus 4 weights\n",
    "# recall that we keep weights transposed\n",
    "weights = np.array([[0.2, 0.8, -0.5, 1],\n",
    " [0.5, -0.91, 0.26, -0.5],\n",
    " [-0.26, -0.27, 0.17, 0.87]]).T\n",
    "# sum weights of given input\n",
    "# and multiply by the passed-in gradient for this neuron\n",
    "dinputs = np.dot(dvalues, weights.T)\n",
    "print(dinputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdd407f",
   "metadata": {},
   "source": [
    "ADDING THE \"BACKWARD\" METHOD IN THE LAYER-DENSE CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7b8ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dense:\n",
    " ...\n",
    " # Backward pass\n",
    " def backward(self, dvalues):\n",
    "    # Gradients on parameters\n",
    "    self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "    self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "    # Gradient on values\n",
    "    self.dinputs = np.dot(dvalues, self.weights.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd1da2d",
   "metadata": {},
   "source": [
    "ADDING THE \"BACKWARD\" METHOD IN THE RELU ACTIVATION CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7bd888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    " # Forward pass\n",
    "    def forward(self, inputs):\n",
    "    # Remember input values\n",
    "        self.inputs = inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    " # Since we need to modify the original variable,\n",
    " # let's make a copy of the values first\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Zero gradient where input values were negative\n",
    "        self.dinputs[self.inputs <= 0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbe1496",
   "metadata": {},
   "source": [
    "LOSS FUNCTION BACKPROPAGATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4f8aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-entropy loss\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    " ...\n",
    "    # Backward pass\n",
    "def backward(self, dvalues, y_true):\n",
    "    # Number of samples\n",
    "    samples = len(dvalues)\n",
    "    # Number of labels in every sample\n",
    "    # We'll use the first sample to count them\n",
    "    labels = len(dvalues[0])\n",
    "    \n",
    "    # If labels are sparse, turn them into one-hot vector\n",
    "    if len(y_true.shape) == 1:\n",
    "        # we are using one-hot encoding\n",
    "        y_true = np.eye(labels)[y_true]\n",
    "        # Calculate gradient\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772a5c3b",
   "metadata": {},
   "source": [
    "Softmax classifier - combined Softmax activation and cross-entropy loss for faster backward step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cd9233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax classifier - combined Softmax activation\n",
    "# and cross-entropy loss for faster backward step\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy:\n",
    "    # Creates activation and loss function objects\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs, y_true):\n",
    "        # Output layer's activation function\n",
    "        self.activation.forward(inputs)\n",
    "        # Set the output\n",
    "        self.output = self.activation.output\n",
    "        # Calculate and return loss value\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # If labels are one-hot encoded,\n",
    "        # turn them into discrete values\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "        # Copy so we can safely modify\n",
    "        self.dinputs = dvalues.copy()\n",
    "        \n",
    "        # Calculate gradient \n",
    "        # wherever the true class is, subtract 1\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74032733",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_outputs = np.array([[0.7, 0.1, 0.2],\n",
    " [0.1, 0.5, 0.4],\n",
    " [0.02, 0.9, 0.08]])\n",
    "class_targets = np.array([0, 1, 1])\n",
    "softmax_loss = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "softmax_loss.backward(softmax_outputs, class_targets)\n",
    "dvalues1 = softmax_loss.dinputs\n",
    "print('Gradients: combined loss and activation:')\n",
    "print(dvalues1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054f77c9",
   "metadata": {},
   "source": [
    "##  Build the entire backpropagation pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d54349",
   "metadata": {},
   "source": [
    "CREATING LAYERS: FORWARD AND BACKWARD PASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203e2815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from input ones, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        # Gradient on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a8616f",
   "metadata": {},
   "source": [
    "ReLU Activation: Forward and Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe21bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Since we need to modify the original variable,\n",
    "        # let’s make a copy of values first\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Zero gradient where input values were negative\n",
    "        self.dinputs[self.inputs <= 0] = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09d8afb",
   "metadata": {},
   "source": [
    "SOFTMAX ACTIVATION: FORWARD PASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf6a0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    " # Forward pass\n",
    " def forward(self, inputs):\n",
    " # Get unnormalized probabilities\n",
    "  exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    " # Normalize them for each sample\n",
    "  probabilities = exp_values / np.sum(exp_values, axis=1,keepdims=True)\n",
    "  self.output = probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335a9b5e",
   "metadata": {},
   "source": [
    "LOSS Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab0a524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common loss class\n",
    "class Loss:\n",
    " # Calculates the data and regularization losses\n",
    " # given model output and ground truth values\n",
    " def calculate(self, output, y):\n",
    "  # Calculate sample losses\n",
    "  sample_losses = self.forward(output, y)\n",
    "  # Calculate mean loss\n",
    "  data_loss = np.mean(sample_losses)\n",
    "  # Return loss\n",
    "  return data_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce5480c",
   "metadata": {},
   "source": [
    "CATEGORICAL CROSS ENTROPY LOSS: FORWARD AND BACKWARD PASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da84a878",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Probabilities for target values -\n",
    "        # only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[\n",
    "                range(samples),\n",
    "                y_true\n",
    "            ]\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(\n",
    "                y_pred_clipped * y_true,\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of labels in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        labels = len(dvalues[0])\n",
    "\n",
    "        # If labels are sparse, turn them into one-hot vector\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "\n",
    "        # Calculate gradient\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5367633b",
   "metadata": {},
   "source": [
    "COMBINED SOFTMAX ACTIVATION AND CATEGORICAL CROSS ENTROPY FOR LAST LAYER: FORWARD AND BACKWARD PASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbae99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax classifier - combined Softmax activation\n",
    "# and cross-entropy loss for faster backward step\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy:\n",
    "    # Creates activation and loss function objects\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs, y_true):\n",
    "        # Output layer's activation function\n",
    "        self.activation.forward(inputs)\n",
    "        # Set the output\n",
    "        self.output = self.activation.output\n",
    "        # Calculate and return loss value\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # If labels are one-hot encoded,\n",
    "        # turn them into discrete values\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "        # Copy so we can safely modify\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d860f16a",
   "metadata": {},
   "source": [
    "FULL CODE UPTO THIS POINT: FORWARD AND BACKWARD PASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9d09ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "# Create Dense layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2, 3)\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "# Create second Dense layer with 3 input features (as we take output\n",
    "# of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense(3, 3)\n",
    "# Create Softmax classifier’s combined loss and activation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "# Perform a forward pass of our training data through this layer\n",
    "dense1.forward(X)\n",
    "# Perform a forward pass through activation function\n",
    "# takes the output of first dense layer here\n",
    "activation1.forward(dense1.output)\n",
    "# Perform a forward pass through second Dense layer\n",
    "# takes outputs of activation function of first layer as inputs\n",
    "dense2.forward(activation1.output)\n",
    "# Perform a forward pass through the activation/loss function\n",
    "# takes the output of second dense layer here and returns loss\n",
    "loss = loss_activation.forward(dense2.output, y)\n",
    "\n",
    "# Let’s see output of the first few samples:\n",
    "print(loss_activation.output[:5])\n",
    "# Print loss value\n",
    "print('loss:', loss)\n",
    "# Calculate accuracy from output of activation2 and targets\n",
    "# calculate values along first axis\n",
    "predictions = np.argmax(loss_activation.output, axis=1)\n",
    "if len(y.shape) == 2:\n",
    " y = np.argmax(y, axis=1)\n",
    "accuracy = np.mean(predictions == y)\n",
    "# Print accuracy\n",
    "print('acc:', accuracy)\n",
    "# Backward pass\n",
    "loss_activation.backward(loss_activation.output, y)\n",
    "dense2.backward(loss_activation.dinputs)\n",
    "activation1.backward(dense2.dinputs)\n",
    "dense1.backward(activation1.dinputs)\n",
    "# Print gradients\n",
    "print(dense1.dweights)\n",
    "print(dense1.dbiases)\n",
    "print(dense2.dweights)\n",
    "print(dense2.dbiases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b72879",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initial parameters\n",
    "weights = np.array([-3.0, -1.0, 2.0])\n",
    "bias = 1.0\n",
    "inputs = np.array([1.0, -2.0, 3.0])\n",
    "target_output = 0.0\n",
    "learning_rate = 0.001\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1.0, 0.0)\n",
    "\n",
    "for iteration in range(200):\n",
    "    # Forward pass\n",
    "    linear_output = np.dot(weights, inputs) + bias\n",
    "    output = relu(linear_output)\n",
    "    loss = (output - target_output) ** 2\n",
    "\n",
    "    # Backward pass\n",
    "    dloss_doutput = 2 * (output - target_output)\n",
    "    doutput_dlinear = relu_derivative(linear_output)\n",
    "    dlinear_dweights = inputs\n",
    "    dlinear_dbias = 1.0\n",
    "\n",
    "    dloss_dlinear = dloss_doutput * doutput_dlinear\n",
    "    dloss_dweights = dloss_dlinear * dlinear_dweights\n",
    "    dloss_dbias = dloss_dlinear * dlinear_dbias\n",
    "\n",
    "    # Update weights and bias\n",
    "    weights -= learning_rate * dloss_dweights\n",
    "    bias -= learning_rate * dloss_dbias\n",
    "\n",
    "    # Print the loss for this iteration\n",
    "    print(f\"Iteration {iteration + 1}, Loss: {loss}\")\n",
    "\n",
    "print(\"Final weights:\", weights)\n",
    "print(\"Final bias:\", bias)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa28574",
   "metadata": {},
   "source": [
    "OPTIMIZERS GRADIENT DESCENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ee8289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD optimizer\n",
    "class Optimizer_SGD:\n",
    " # Initialize optimizer - set settings,\n",
    " # learning rate of 1. is default for this optimizer\n",
    " def __init__(self, learning_rate=0.5):\n",
    "  self.learning_rate = learning_rate\n",
    " # Update parameters\n",
    " def update_params(self, layer):\n",
    "  layer.weights += -self.learning_rate * layer.dweights\n",
    "  layer.biases += -self.learning_rate * layer.dbiases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77285d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# Create Dense layer with 2 input features and 64 output values\n",
    "dense1 = Layer_Dense(2, 64)\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer)\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 64 input features (as we take output\n",
    "# of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense(64, 3)\n",
    "\n",
    "# Create Softmax classifier's combined loss and activation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = Optimizer_SGD()\n",
    "\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "    # Perform a forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "    \n",
    "    # Perform a forward pass through activation function\n",
    "    # takes the output of first dense layer here\n",
    "    activation1.forward(dense1.output)\n",
    "    \n",
    "    # Perform a forward pass through second Dense layer\n",
    "    # takes outputs of activation function of first layer as inputs\n",
    "    dense2.forward(activation1.output)\n",
    "    \n",
    "    # Perform a forward pass through the activation/loss function\n",
    "    # takes the output of second dense layer here and returns loss\n",
    "    loss = loss_activation.forward(dense2.output, y)\n",
    "    \n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    # calculate values along first axis\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    \n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' +\n",
    "              f'acc: {accuracy:.3f}, ' +\n",
    "              f'loss: {loss:.3f}')\n",
    "    \n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    \n",
    "    # Update weights and biases\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db982b1",
   "metadata": {},
   "source": [
    "OPTIMIZERS: LEARNING RATE DECAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12047a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_SGD:\n",
    "    # Initialize optimizer - set settings,\n",
    "    # learning rate of 1. is default for this optimizer\n",
    "    def __init__(self, learning_rate=1., decay=0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        layer.weights += -self.current_learning_rate * layer.dweights\n",
    "        layer.biases += -self.current_learning_rate * layer.dbiases\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd9e8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming the necessary classes (Layer_Dense, Activation_ReLU, \n",
    "# Activation_Softmax_Loss_CategoricalCrossentropy, and spiral_data) are defined elsewhere\n",
    "\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# Create Dense layer with 2 input features and 64 output values\n",
    "dense1 = Layer_Dense(2, 64)\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer)\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 64 input features (as we take output\n",
    "# of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense(64, 3)\n",
    "\n",
    "# Create Softmax classifier's combined loss and activation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = Optimizer_SGD(decay=1e-3)\n",
    "\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "    # Perform a forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "    \n",
    "    # Perform a forward pass through activation function\n",
    "    # takes the output of first dense layer here\n",
    "    activation1.forward(dense1.output)\n",
    "    \n",
    "    # Perform a forward pass through second Dense layer\n",
    "    # takes outputs of activation function of first layer as inputs\n",
    "    dense2.forward(activation1.output)\n",
    "    \n",
    "    # Perform a forward pass through the activation/loss function\n",
    "    # takes the output of second dense layer here and returns loss\n",
    "    loss = loss_activation.forward(dense2.output, y)\n",
    "    \n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    # calculate values along first axis\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    \n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' +\n",
    "              f'acc: {accuracy:.3f}, ' +\n",
    "              f'loss: {loss:.3f}, ' +\n",
    "              f'lr: {optimizer.current_learning_rate}')\n",
    "    \n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    \n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a568134b",
   "metadata": {},
   "source": [
    "## Lecture 24 - Momentum in training neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339a9657",
   "metadata": {},
   "source": [
    "OPTIMIZERS: MOMENTUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba030886",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Optimizer_SGD:\n",
    "    # Initialize optimizer - set settings,\n",
    "    # learning rate of 1. is default for this optimizer\n",
    "    def __init__(self, learning_rate=1., decay=0., momentum=0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        # If we use momentum\n",
    "        if self.momentum:\n",
    "            # If layer does not contain momentum arrays, create them\n",
    "            # filled with zeros\n",
    "            if not hasattr(layer, 'weight_momentums'):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights) # previous weight updates\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases) # previous bias updates\n",
    "\n",
    "            # Build weight updates with momentum - take previous\n",
    "            # updates multiplied by retain factor and update with\n",
    "            # current gradients\n",
    "            weight_updates = self.momentum * layer.weight_momentums - \\\n",
    "                             self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "\n",
    "            # Build bias updates\n",
    "            bias_updates = self.momentum * layer.bias_momentums - \\\n",
    "                           self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentums = bias_updates\n",
    "\n",
    "        # Vanilla SGD updates (as before momentum update)\n",
    "        else:\n",
    "            weight_updates = -self.current_learning_rate * layer.dweights\n",
    "            bias_updates = -self.current_learning_rate * layer.dbiases\n",
    "\n",
    "        # Update weights and biases using either\n",
    "        # vanilla or momentum updates\n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3fe78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming the necessary classes (Layer_Dense, Activation_ReLU, \n",
    "# Activation_Softmax_Loss_CategoricalCrossentropy, Optimizer_SGD, and spiral_data) are defined elsewhere\n",
    "\n",
    "# Create dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# Create Dense layer with 2 input features and 64 output values\n",
    "dense1 = Layer_Dense(2, 64)\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer)\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 64 input features (as we take output\n",
    "# of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense(64, 3)\n",
    "\n",
    "# Create Softmax classifier's combined loss and activation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = Optimizer_SGD(decay=1e-3, momentum=0.9)\n",
    "\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "    # Perform a forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "    \n",
    "    # Perform a forward pass through activation function\n",
    "    # takes the output of first dense layer here\n",
    "    activation1.forward(dense1.output)\n",
    "    \n",
    "    # Perform a forward pass through second Dense layer\n",
    "    # takes outputs of activation function of first layer as inputs\n",
    "    dense2.forward(activation1.output)\n",
    "    \n",
    "    # Perform a forward pass through the activation/loss function\n",
    "    # takes the output of second dense layer here and returns loss\n",
    "    loss = loss_activation.forward(dense2.output, y)\n",
    "    \n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    # calculate values along first axis\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    \n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' +\n",
    "              f'acc: {accuracy:.3f}, ' +\n",
    "              f'loss: {loss:.3f}, ' +\n",
    "              f'lr: {optimizer.current_learning_rate}')\n",
    "    \n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    \n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de2858a",
   "metadata": {},
   "source": [
    "OPTIMIZERS: ADAGRAD\n",
    "\n",
    "Adaptive Learning Rates\n",
    "\n",
    "Adagrad adapts the learning rate differently for each parameter\n",
    "Parameters that receive more updates get smaller learning rates\n",
    "Parameters that receive fewer updates get larger learning rates\n",
    "This is particularly useful when dealing with sparse data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c876cc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Adagrad optimizer\n",
    "class Optimizer_Adagrad:\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=1., decay=0., epsilon=1e-7):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        # If layer does not contain cache arrays, create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache += layer.dweights**2\n",
    "        layer.bias_cache += layer.dbiases**2\n",
    "\n",
    "        # Vanilla SGD parameter update + normalization with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * \\\n",
    "            layer.dweights / \\\n",
    "            (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * \\\n",
    "            layer.dbiases / \\\n",
    "            (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582b410b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# Create Dense layer with 2 input features and 64 output values\n",
    "dense1 = Layer_Dense(2, 64)\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer)\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 64 input features (as we take output of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense(64, 3)\n",
    "\n",
    "# Create Softmax classifier's combined loss and activation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Create optimizer\n",
    "# optimizer = Optimizer_SGD(decay=8e-8, momentum=0.9)\n",
    "optimizer = Optimizer_Adagrad(decay=1e-4)\n",
    "\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "    # Perform a forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "    \n",
    "    # Perform a forward pass through activation function\n",
    "    # takes the output of first dense layer here\n",
    "    activation1.forward(dense1.output)\n",
    "    \n",
    "    # Perform a forward pass through second Dense layer\n",
    "    # takes outputs of activation function of first layer as inputs\n",
    "    dense2.forward(activation1.output)\n",
    "    \n",
    "    # Perform a forward pass through the activation/loss function\n",
    "    # takes the output of second dense layer here and returns loss\n",
    "    loss = loss_activation.forward(dense2.output, y)\n",
    "    \n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    # calculate values along first axis\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    \n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' +\n",
    "              f'acc: {accuracy:.3f}, ' +\n",
    "              f'loss: {loss:.3f}, ' +\n",
    "              f'lr: {optimizer.current_learning_rate}')\n",
    "    \n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    \n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356d0d3a",
   "metadata": {},
   "source": [
    "OPTIMIZERS: RMSPROP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b78ba3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Optimizer_RMSprop:\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7, rho=0.9):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.rho = rho\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        # If layer does not contain cache arrays,\n",
    "        # create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache = self.rho * layer.weight_cache + \\\n",
    "                             (1 - self.rho) * layer.dweights**2\n",
    "        layer.bias_cache = self.rho * layer.bias_cache + \\\n",
    "                           (1 - self.rho) * layer.dbiases**2\n",
    "\n",
    "        # Vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * \\\n",
    "                         layer.dweights / \\\n",
    "                         (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * \\\n",
    "                        layer.dbiases / \\\n",
    "                        (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f830535",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# Create Dense layer with 2 input features and 64 output values\n",
    "dense1 = Layer_Dense(2, 64)\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer)\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 64 input features (as we take output of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense(64, 3)\n",
    "\n",
    "# Create Softmax classifier's combined loss and activation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Create optimizer\n",
    "# optimizer = Optimizer_SGD(decay=8e-8, momentum=0.9)\n",
    "#optimizer = Optimizer_Adagrad(decay=1e-4)\n",
    "#optimizer = Optimizer_RMSprop(decay=1e-4)\n",
    "optimizer = Optimizer_RMSprop(learning_rate=0.02, decay=1e-5,rho=0.999)\n",
    "\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "    # Perform a forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "\n",
    "    # Perform a forward pass through activation function\n",
    "    # takes the output of first dense layer here\n",
    "    activation1.forward(dense1.output)\n",
    "\n",
    "    # Perform a forward pass through second Dense layer\n",
    "    # takes outputs of activation function of first layer as inputs\n",
    "    dense2.forward(activation1.output)\n",
    "\n",
    "    # Perform a forward pass through the activation/loss function\n",
    "    # takes the output of second dense layer here and returns loss\n",
    "    loss = loss_activation.forward(dense2.output, y)\n",
    "\n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    # calculate values along first axis\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions == y)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' +\n",
    "              f'acc: {accuracy:.3f}, ' +\n",
    "              f'loss: {loss:.3f}, ' +\n",
    "              f'lr: {optimizer.current_learning_rate}')\n",
    "\n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ea2723",
   "metadata": {},
   "source": [
    "OPTIMIZERS: ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607f89a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Adam optimizer\n",
    "class Optimizer_Adam:\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7, beta_1=0.9, beta_2=0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        # If layer does not contain cache arrays, create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update momentum with current gradients\n",
    "        layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1 - self.beta_1) * layer.dweights\n",
    "        layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1 - self.beta_1) * layer.dbiases\n",
    "\n",
    "        # Get corrected momentum\n",
    "        # self.iteration is 0 at first pass and we need to start with 1 here\n",
    "        weight_momentums_corrected = layer.weight_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        bias_momentums_corrected = layer.bias_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "\n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache + (1 - self.beta_2) * layer.dweights**2\n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + (1 - self.beta_2) * layer.dbiases**2\n",
    "\n",
    "        # Get corrected cache\n",
    "        weight_cache_corrected = layer.weight_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        bias_cache_corrected = layer.bias_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "\n",
    "        # Vanilla SGD parameter update + normalization with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * weight_momentums_corrected / (np.sqrt(weight_cache_corrected) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * bias_momentums_corrected / (np.sqrt(bias_cache_corrected) + self.epsilon)\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "94cd3336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.297, loss: 1.099, lr: 0.02\n",
      "epoch: 100, acc: 0.583, loss: 0.895, lr: 0.01998021958261321\n",
      "epoch: 200, acc: 0.730, loss: 0.661, lr: 0.019960279044701046\n",
      "epoch: 300, acc: 0.817, loss: 0.530, lr: 0.019940378268975763\n",
      "epoch: 400, acc: 0.830, loss: 0.461, lr: 0.01992051713662487\n",
      "epoch: 500, acc: 0.847, loss: 0.416, lr: 0.01990069552930875\n",
      "epoch: 600, acc: 0.853, loss: 0.387, lr: 0.019880913329158343\n",
      "epoch: 700, acc: 0.880, loss: 0.366, lr: 0.019861170418772778\n",
      "epoch: 800, acc: 0.867, loss: 0.348, lr: 0.019841466681217078\n",
      "epoch: 900, acc: 0.867, loss: 0.337, lr: 0.01982180200001982\n",
      "epoch: 1000, acc: 0.887, loss: 0.322, lr: 0.019802176259170884\n",
      "epoch: 1100, acc: 0.873, loss: 0.321, lr: 0.01978258934311912\n",
      "epoch: 1200, acc: 0.880, loss: 0.305, lr: 0.01976304113677013\n",
      "epoch: 1300, acc: 0.893, loss: 0.295, lr: 0.019743531525483964\n",
      "epoch: 1400, acc: 0.883, loss: 0.288, lr: 0.01972406039507293\n",
      "epoch: 1500, acc: 0.900, loss: 0.281, lr: 0.019704627631799327\n",
      "epoch: 1600, acc: 0.900, loss: 0.276, lr: 0.019685233122373254\n",
      "epoch: 1700, acc: 0.903, loss: 0.271, lr: 0.019665876753950384\n",
      "epoch: 1800, acc: 0.910, loss: 0.263, lr: 0.01964655841412981\n",
      "epoch: 1900, acc: 0.910, loss: 0.256, lr: 0.019627277990951823\n",
      "epoch: 2000, acc: 0.913, loss: 0.255, lr: 0.019608035372895814\n",
      "epoch: 2100, acc: 0.917, loss: 0.249, lr: 0.01958883044887805\n",
      "epoch: 2200, acc: 0.920, loss: 0.245, lr: 0.019569663108249594\n",
      "epoch: 2300, acc: 0.917, loss: 0.241, lr: 0.01955053324079414\n",
      "epoch: 2400, acc: 0.920, loss: 0.239, lr: 0.019531440736725945\n",
      "epoch: 2500, acc: 0.907, loss: 0.235, lr: 0.019512385486687673\n",
      "epoch: 2600, acc: 0.917, loss: 0.232, lr: 0.019493367381748363\n",
      "epoch: 2700, acc: 0.923, loss: 0.230, lr: 0.019474386313401298\n",
      "epoch: 2800, acc: 0.920, loss: 0.227, lr: 0.019455442173562\n",
      "epoch: 2900, acc: 0.913, loss: 0.225, lr: 0.019436534854566128\n",
      "epoch: 3000, acc: 0.920, loss: 0.222, lr: 0.01941766424916747\n",
      "epoch: 3100, acc: 0.913, loss: 0.219, lr: 0.019398830250535893\n",
      "epoch: 3200, acc: 0.917, loss: 0.217, lr: 0.019380032752255354\n",
      "epoch: 3300, acc: 0.920, loss: 0.217, lr: 0.01936127164832186\n",
      "epoch: 3400, acc: 0.920, loss: 0.218, lr: 0.01934254683314152\n",
      "epoch: 3500, acc: 0.917, loss: 0.211, lr: 0.019323858201528515\n",
      "epoch: 3600, acc: 0.920, loss: 0.210, lr: 0.019305205648703173\n",
      "epoch: 3700, acc: 0.930, loss: 0.207, lr: 0.01928658907028997\n",
      "epoch: 3800, acc: 0.917, loss: 0.206, lr: 0.01926800836231563\n",
      "epoch: 3900, acc: 0.927, loss: 0.206, lr: 0.019249463421207133\n",
      "epoch: 4000, acc: 0.917, loss: 0.202, lr: 0.019230954143789846\n",
      "epoch: 4100, acc: 0.923, loss: 0.200, lr: 0.019212480427285565\n",
      "epoch: 4200, acc: 0.927, loss: 0.198, lr: 0.019194042169310647\n",
      "epoch: 4300, acc: 0.923, loss: 0.196, lr: 0.019175639267874092\n",
      "epoch: 4400, acc: 0.920, loss: 0.195, lr: 0.019157271621375684\n",
      "epoch: 4500, acc: 0.927, loss: 0.191, lr: 0.0191389391286041\n",
      "epoch: 4600, acc: 0.923, loss: 0.188, lr: 0.019120641688735073\n",
      "epoch: 4700, acc: 0.933, loss: 0.186, lr: 0.019102379201329525\n",
      "epoch: 4800, acc: 0.917, loss: 0.191, lr: 0.01908415156633174\n",
      "epoch: 4900, acc: 0.933, loss: 0.184, lr: 0.01906595868406753\n",
      "epoch: 5000, acc: 0.923, loss: 0.183, lr: 0.01904780045524243\n",
      "epoch: 5100, acc: 0.930, loss: 0.182, lr: 0.019029676780939874\n",
      "epoch: 5200, acc: 0.937, loss: 0.180, lr: 0.019011587562619416\n",
      "epoch: 5300, acc: 0.940, loss: 0.179, lr: 0.01899353270211493\n",
      "epoch: 5400, acc: 0.940, loss: 0.178, lr: 0.018975512101632844\n",
      "epoch: 5500, acc: 0.940, loss: 0.177, lr: 0.018957525663750367\n",
      "epoch: 5600, acc: 0.930, loss: 0.180, lr: 0.018939573291413745\n",
      "epoch: 5700, acc: 0.927, loss: 0.176, lr: 0.018921654887936498\n",
      "epoch: 5800, acc: 0.927, loss: 0.175, lr: 0.018903770356997706\n",
      "epoch: 5900, acc: 0.930, loss: 0.174, lr: 0.018885919602640248\n",
      "epoch: 6000, acc: 0.923, loss: 0.177, lr: 0.018868102529269144\n",
      "epoch: 6100, acc: 0.933, loss: 0.171, lr: 0.018850319041649778\n",
      "epoch: 6200, acc: 0.930, loss: 0.176, lr: 0.018832569044906263\n",
      "epoch: 6300, acc: 0.940, loss: 0.170, lr: 0.018814852444519702\n",
      "epoch: 6400, acc: 0.927, loss: 0.169, lr: 0.018797169146326564\n",
      "epoch: 6500, acc: 0.940, loss: 0.169, lr: 0.01877951905651696\n",
      "epoch: 6600, acc: 0.933, loss: 0.167, lr: 0.018761902081633034\n",
      "epoch: 6700, acc: 0.923, loss: 0.172, lr: 0.018744318128567278\n",
      "epoch: 6800, acc: 0.940, loss: 0.166, lr: 0.018726767104560903\n",
      "epoch: 6900, acc: 0.933, loss: 0.165, lr: 0.018709248917202218\n",
      "epoch: 7000, acc: 0.933, loss: 0.165, lr: 0.018691763474424996\n",
      "epoch: 7100, acc: 0.940, loss: 0.164, lr: 0.018674310684506857\n",
      "epoch: 7200, acc: 0.937, loss: 0.163, lr: 0.01865689045606769\n",
      "epoch: 7300, acc: 0.927, loss: 0.164, lr: 0.01863950269806802\n",
      "epoch: 7400, acc: 0.933, loss: 0.162, lr: 0.018622147319807447\n",
      "epoch: 7500, acc: 0.920, loss: 0.165, lr: 0.018604824230923075\n",
      "epoch: 7600, acc: 0.930, loss: 0.160, lr: 0.01858753334138793\n",
      "epoch: 7700, acc: 0.937, loss: 0.160, lr: 0.018570274561509396\n",
      "epoch: 7800, acc: 0.937, loss: 0.168, lr: 0.018553047801927663\n",
      "epoch: 7900, acc: 0.940, loss: 0.160, lr: 0.018535852973614212\n",
      "epoch: 8000, acc: 0.937, loss: 0.156, lr: 0.01851868998787026\n",
      "epoch: 8100, acc: 0.937, loss: 0.155, lr: 0.018501558756325222\n",
      "epoch: 8200, acc: 0.943, loss: 0.154, lr: 0.01848445919093522\n",
      "epoch: 8300, acc: 0.943, loss: 0.153, lr: 0.018467391203981567\n",
      "epoch: 8400, acc: 0.937, loss: 0.154, lr: 0.018450354708069265\n",
      "epoch: 8500, acc: 0.930, loss: 0.153, lr: 0.018433349616125496\n",
      "epoch: 8600, acc: 0.927, loss: 0.153, lr: 0.018416375841398172\n",
      "epoch: 8700, acc: 0.930, loss: 0.152, lr: 0.01839943329745444\n",
      "epoch: 8800, acc: 0.940, loss: 0.150, lr: 0.01838252189817921\n",
      "epoch: 8900, acc: 0.940, loss: 0.150, lr: 0.018365641557773718\n",
      "epoch: 9000, acc: 0.940, loss: 0.150, lr: 0.018348792190754044\n",
      "epoch: 9100, acc: 0.940, loss: 0.149, lr: 0.0183319737119497\n",
      "epoch: 9200, acc: 0.940, loss: 0.149, lr: 0.018315186036502167\n",
      "epoch: 9300, acc: 0.933, loss: 0.148, lr: 0.018298429079863496\n",
      "epoch: 9400, acc: 0.940, loss: 0.147, lr: 0.018281702757794862\n",
      "epoch: 9500, acc: 0.940, loss: 0.150, lr: 0.018265006986365174\n",
      "epoch: 9600, acc: 0.943, loss: 0.147, lr: 0.018248341681949654\n",
      "epoch: 9700, acc: 0.940, loss: 0.146, lr: 0.018231706761228456\n",
      "epoch: 9800, acc: 0.930, loss: 0.147, lr: 0.018215102141185255\n",
      "epoch: 9900, acc: 0.937, loss: 0.145, lr: 0.018198527739105907\n",
      "epoch: 10000, acc: 0.937, loss: 0.145, lr: 0.018181983472577025\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# Create Dense layer with 2 input features and 64 output values\n",
    "dense1 = Layer_Dense(2, 64)\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer)\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 64 input features (as we take output of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense(64, 3)\n",
    "\n",
    "# Create Softmax classifier's combined loss and activation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Create optimizer\n",
    "#optimizer = Optimizer_SGD(decay=8e-8, momentum=0.9)\n",
    "#optimizer = Optimizer_Adagrad(decay=1e-4)\n",
    "#optimizer = Optimizer_RMSprop(decay=1e-4)\n",
    "#optimizer = Optimizer_RMSprop(learning_rate=0.02, decay=1e-5,rho=0.999)\n",
    "\n",
    "optimizer = Optimizer_Adam(learning_rate=0.02, decay=1e-5)\n",
    "\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "    # Perform a forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "\n",
    "    # Perform a forward pass through activation function\n",
    "    # takes the output of first dense layer here\n",
    "    activation1.forward(dense1.output)\n",
    "\n",
    "    # Perform a forward pass through second Dense layer\n",
    "    # takes outputs of activation function of first layer as inputs\n",
    "    dense2.forward(activation1.output)\n",
    "\n",
    "    # Perform a forward pass through the activation/loss function\n",
    "    # takes the output of second dense layer here and returns loss\n",
    "    loss = loss_activation.forward(dense2.output, y)\n",
    "\n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    # calculate values along first axis\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions == y)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' +\n",
    "              f'acc: {accuracy:.3f}, ' +\n",
    "              f'loss: {loss:.3f}, ' +\n",
    "              f'lr: {optimizer.current_learning_rate}')\n",
    "\n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740c7a4f",
   "metadata": {},
   "source": [
    "#ADAM: 0.957\n",
    "#RMSPROP: 0.717\n",
    "#MOMENTUM: 0.873"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9259f6cb",
   "metadata": {},
   "source": [
    "## -Neural network testing, generalization and overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927f933c",
   "metadata": {},
   "source": [
    "TESTING WITH OUT OF SAMPLE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "c0f9d6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation, acc: 0.780, loss: 1.137\n"
     ]
    }
   ],
   "source": [
    "# Validate the model\n",
    "# Create test dataset\n",
    "X_test, y_test = spiral_data(samples=100, classes=3)\n",
    "# Perform a forward pass of our testing data through this layer\n",
    "dense1.forward(X_test)\n",
    "# Perform a forward pass through activation function\n",
    "# takes the output of first dense layer here\n",
    "activation1.forward(dense1.output)\n",
    "# Perform a forward pass through second Dense layer\n",
    "# takes outputs of activation function of first layer as inputs\n",
    "dense2.forward(activation1.output)\n",
    "# Perform a forward pass through the activation/loss function\n",
    "# takes the output of second dense layer here and returns loss\n",
    "loss = loss_activation.forward(dense2.output, y_test)\n",
    "# Calculate accuracy from output of activation2 and targets\n",
    "# calculate values along first axis\n",
    "predictions = np.argmax(loss_activation.output, axis=1)\n",
    "if len(y_test.shape) == 2:\n",
    " y_test = np.argmax(y_test, axis=1)\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "print(f'validation, acc: {accuracy:.3f}, loss: {loss:.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
